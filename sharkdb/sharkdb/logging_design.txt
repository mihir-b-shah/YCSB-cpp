
Just a side note, make sure we don't hold namespace locks for too long- should be fine, what we are
doing now, where we hold them during a R/W op?

Problems with the current design:
- We fsync on every operation. We can change this to fsync on every write batch submitted (Commit 61118d). This is a baseline to measure logging perf (throughput, vs latency)
- The MAIN benefit of RocksDB's logging scheme is READ performance- i.e. that readers do not block
  writers. This "sounds" like MVCC in a sequentially consistent system- but it's not- two writes to the
  same key can be logged together. Note we still get reasonable semantics- if I write a value myself,
  it will be persisted and then return to me- so a future read I do WILL read that value.
- As such, we want to measure read perf. We should do this on a highly skewed dataset- so that it
  maximally stresses the reads on a small (and cacheable) set- so I can essentially ignore compaction,
  to make my life simple. 

It is obvious my system will get very poor R performance on a skewed dataset. Consider every op is on the
same key. Then on every write in the current system, we will block readers. Suppose we have workload A
from ycsb- this means for each log op, while we hold the lock, no reads will proceed.

-------------
To implement
-------------

Write path:
- write to log buffer
	- no need for a separate logging thread. Just have a method, that the foreground threads run. The 
	  critical thing is for it to be O(1).
	- the l0 object should maintain a log buffer.
	- how to manage the log buffer? we append to it to mirror what we will write to the WAL. However,
	  for a heavy contention workload, just like the log-structured FS, lots of stuff will get
	  overwritten, as such we will make poor use of the log_buffer. This allows using overwritten slots
	  as extra cache space.
- update memtable's mvcc data structure that points to the log buffer
	- how should this be maintained? maintain an epoch number (rather than a full timestamp).
	  for any key, I need to maintain two things- a pointer to the last committed state, and to
	  the last un-committed state.
	- suppose a read comes in. At that point, it should use the "last committed state" pointer. Once
	  there, it has a pointer directly to the log buffer, which is live until mem-table flush- regardless
	  of future writes that come in.
	- suppose a write comes in. If there is a current un-committed write, just replace it, nobody can
	  read it anyway. 
- push completion event into the private queue
	- maintain a private queue where we write completions.

Read path
- sync for now

Sources of I/O (all separate rings)
- log writes/fsync- this one doesn't need polling.
- block reads- lots of I/O- possibly interdependent- interesting question on how to implement-
    do we readahead 2 blocks, or just do one at a time, etc? Effect on i/o queues, etc? SQ/CQ
    ops should be vv fast, i.e. polled. 
- flush to sstable -> separate ring- this is less latency critical, but nevertheless polled I/O
    might be nice. Note this is a completely different situation than block reads, though.

Async operation:
- Note there were 3 phases to deleting a full memtable.
(1) memtable is W mode by users, - by flush thread.
(2) memtable is swapped out- and not modified more- by the user, and we start flushing it to disk.
(3) memtable swp read-only version for users is removed, and the finished SSTable is made visible.

Now there is a catch- we want to wait for all operations to complete on the memtable before we start
flushing it. (This is not strictly necessary, we could perhaps overlap certain things, but for simplicity
let's just do this). So after we swap out the memtable, no NEW user ops should be able to use the memtable
for anything- it will just be the existing I/O on the memtable resolving. As such, we must wait for all
such I/O to resolve. What kinds of I/O can this be?
- None at all! All memtable writes happen synchronously- as do all memtable reads.

What about log writes- when we swap a l0 object out, the wal switches too. But this means any pending
fsyncs and log writes can just happen, and they'll be absorbed by the swapped out WAL (since no one
is using it). This is due to a quirk in our impl, that probably wastes a good deal of SSD space, but
for now it's probably fine.

What does the io_mgmt thread look like?
- Maintain an io_ring for 2 different tasks- log-flushing and reads.
- At the start of every loop, just check if there are logs to flush, and do it. It is very simple,
    I don't need a polling thread for it, etc. Of course, also check the CQ for that ring to see
    if anything resolved.
- Now the real fun- the read io_ring.
- This will used polled I/O- which creates a background kernel thread to listen for my submissions.
    Look for SQPOLL for sq, IOPOLL for cq. There is also SQ_AFF to specify sq_thread_cpu, to tell kernel
    which cpu to pin sq polling to. IOPOLL is only legal for O_DIRECT files, means I probably should
    manage a user-space cache?
